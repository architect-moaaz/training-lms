{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86af2329",
   "metadata": {},
   "source": [
    "# Day 2 â€” Communication & Automation Agent (LangGraph)\n",
    "\n",
    "This notebook implements a Communication & Automation Agent using:\n",
    "- LangGraph (agent orchestration)\n",
    "- LangChain (LLM + tools)\n",
    "\n",
    "**Capabilities:**\n",
    "- Analyze a message (tone, emotion, intent, urgency)\n",
    "- Classify message (Complaint / Request / Escalation / Query / Feedback / Internal)\n",
    "- Generate a professional reply\n",
    "- Extract tasks (who / what / when / priority)\n",
    "\n",
    "**Architecture:**\n",
    "`HumanMessage â†’ LLM (with tools) â†’ Tools (classify / reply / tasks) â†’ LLM â†’ Final answer`\n",
    "\n",
    "You can later wrap `run_communication_agent()` as a `/communication-agent` API in FastAPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f565b",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies (if needed)\n",
    "Uncomment and run this cell once in your environment (Colab / local).\n",
    "- langgraph\n",
    "- langchain\n",
    "- langchain-openai\n",
    "- pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bd835",
   "metadata": {},
   "outputs": [],
   "source": "# !pip install \"langgraph>=1.0.0\" \"langchain>=0.3.0\" \"langchain-openai\" \"pydantic[email]\" \"grandalf\" \"matplotlib\""
  },
  {
   "cell_type": "markdown",
   "id": "943be50b",
   "metadata": {},
   "source": [
    "## 1. Imports & LLM Setup\n",
    "Make sure you have your OpenAI-compatible key set:\n",
    "`export OPENAI_API_KEY=\"sk-...\"`\n",
    "\n",
    "You can replace the model with any OpenAI-compatible one (e.g. `gpt-4.1`, `gpt-4o`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2e132",
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Dict, Any\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nfrom pydantic import BaseModel, Field\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import tool\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage, SystemMessage, ToolMessage\n\nfrom langgraph.graph import StateGraph, START, END\nfrom typing import Literal\nimport os\n\n# Set your OpenAI API key here or via environment variable\n# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n# Or set it in your shell: export OPENAI_API_KEY=\"sk-...\"\n\n# Initialize the model\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",  # change if you want a different model\n    temperature=0.2,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "04cba736",
   "metadata": {},
   "source": [
    "## 2. Define Pydantic Models for Tool I/O\n",
    "\n",
    "We define:\n",
    "- `ClassificationResult`: category, urgency, tone, emotion, intent\n",
    "- `ReplyInput`: input for reply_generation tool\n",
    "- `Task`, `TaskExtractionResult`: outputs from task extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63237b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationResult(BaseModel):\n",
    "    category: str = Field(\n",
    "        description=\"High-level category of the message (Complaint, Request, Escalation, Query, Feedback, Internal)\"\n",
    "    )\n",
    "    urgency: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"Urgency score from 1 (low) to 5 (critical)\"\n",
    "    )\n",
    "    tone: str = Field(\n",
    "        description=\"Perceived tone (neutral, polite, angry, frustrated, thankful, etc.)\"\n",
    "    )\n",
    "    emotion: str = Field(\n",
    "        description=\"Primary emotion detected (frustration, confusion, satisfaction, etc.)\"\n",
    "    )\n",
    "    intent: str = Field(\n",
    "        description=\"Short description of the main intent of the message\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ReplyInput(BaseModel):\n",
    "    message: str = Field(description=\"Original message from the sender.\")\n",
    "    classification: ClassificationResult = Field(\n",
    "        description=\"Classification result of the message.\"\n",
    "    )\n",
    "    style: str = Field(\n",
    "        default=\"professional and empathetic\",\n",
    "        description=\"Tone/style for the reply, e.g. 'professional and empathetic'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Task(BaseModel):\n",
    "    assignee: str = Field(\n",
    "        description=\"Who should do the task (if not specified, use 'Unassigned').\"\n",
    "    )\n",
    "    description: str = Field(description=\"What needs to be done.\")\n",
    "    due_date: str = Field(\n",
    "        description=\"Due date if explicitly mentioned, else 'Not specified'.\"\n",
    "    )\n",
    "    priority: str = Field(\n",
    "        description=\"Priority label (Low, Medium, High, Critical).\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TaskExtractionResult(BaseModel):\n",
    "    tasks: List[Task] = Field(\n",
    "        description=\"List of extracted tasks from the message.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c17d95",
   "metadata": {},
   "source": [
    "## 3. Define Tools\n",
    "\n",
    "We create 3 tools to match your Day 2 learning goals:\n",
    "\n",
    "1. `classify_message_tool` â€“ Category, urgency, tone, emotion, intent\n",
    "2. `generate_reply_tool` â€“ Professional reply based on classification\n",
    "3. `extract_tasks_tool` â€“ Task extraction (assignee, description, due_date, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38155de4",
   "metadata": {},
   "outputs": [],
   "source": "@tool(parse_docstring=True)\ndef classify_message_tool(message: str) -> ClassificationResult:\n    \"\"\"\\\n    Analyze a message and return:\n    - category\n    - urgency (1-5)\n    - tone\n    - emotion\n    - intent\n\n    Args:\n        message: The raw text message from user/client.\n    \"\"\"\n    prompt = f\"\"\"\nYou are a message classification assistant.\n\nAnalyze the following message and output a JSON with:\n- category: one of [\"Complaint\", \"Request\", \"Escalation\", \"Query\", \"Feedback\", \"Internal\"]\n- urgency: integer 1 (low) to 5 (critical)\n- tone: a short label for tone\n- emotion: primary emotion\n- intent: short description of what they want\n\nMessage:\n{message}\n\"\"\"\n    model = llm.with_structured_output(ClassificationResult)\n    result = model.invoke(prompt)\n    return result\n\n\n@tool(parse_docstring=True)\ndef generate_reply_tool(data: ReplyInput) -> str:\n    \"\"\"\\\n    Generate a professional reply to a message using its classification.\n\n    Args:\n        data: ReplyInput object containing message, classification, and style preferences.\n    \"\"\"\n    cls = data.classification\n    prompt = f\"\"\"\nYou are an assistant writing replies to messages.\n\nWrite a {data.style} reply to the message below.\n\nMessage:\n{data.message}\n\nClassification:\n- Category: {cls.category}\n- Urgency: {cls.urgency}\n- Tone: {cls.tone}\n- Emotion: {cls.emotion}\n- Intent: {cls.intent}\n\nGuidelines:\n- Be clear and concise.\n- Acknowledge the sender's emotion if negative.\n- If it's a complaint or escalation, apologize where appropriate.\n- If it is a request or query, provide clarity or next steps.\n- Do not invent details you don't know.\n\"\"\"\n    response = llm.invoke(prompt)\n    return response.content\n\n\n@tool(parse_docstring=True)\ndef extract_tasks_tool(message: str) -> TaskExtractionResult:\n    \"\"\"\\\n    Extract tasks from a message.\n\n    Args:\n        message: A message that may contain implied or explicit tasks.\n    \"\"\"\n    prompt = f\"\"\"\nYou are a task extraction assistant.\n\nFrom the message below, extract a list of tasks.\nFor each task, identify:\n- assignee (if present, else 'Unassigned')\n- description\n- due_date (if mentioned, else 'Not specified')\n- priority (Low / Medium / High / Critical)\n\nReturn only valid tasks; if none, return an empty list.\n\nMessage:\n{message}\n\"\"\"\n    model = llm.with_structured_output(TaskExtractionResult)\n    result = model.invoke(prompt)\n    return result\n\n\ntools = [classify_message_tool, generate_reply_tool, extract_tasks_tool]\ntools_by_name = {t.name: t for t in tools}\n\nmodel_with_tools = llm.bind_tools(tools)"
  },
  {
   "cell_type": "markdown",
   "id": "8ecc1bcd",
   "metadata": {},
   "source": [
    "## 4. Define LangGraph State\n",
    "\n",
    "We'll use a state that holds:\n",
    "- `messages`: list of LangChain messages (Human, AI, Tool)\n",
    "- `llm_calls`: count of LLM invocations (for debugging/teaching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3993f",
   "metadata": {},
   "source": [
    "## 5. Define LLM Node\n",
    "\n",
    "Node name: `llm_node`\n",
    "\n",
    "Responsibilities:\n",
    "- Add a system message with instructions\n",
    "- Call LLM with bound tools\n",
    "- Append `AIMessage` to the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621af891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"LLM decides whether to respond directly or call tools.\"\"\"\n",
    "    system = SystemMessage(\n",
    "        content=(\n",
    "            \"You are a Communication & Automation Agent.\\n\"\n",
    "            \"You analyze incoming messages, classify them, generate professional replies, \"\n",
    "            \"and extract tasks.\\n\"\n",
    "            \"Use the available tools when you need to classify, generate a reply, \"\n",
    "            \"or extract tasks.\\n\"\n",
    "            \"If tools have already been used and you have enough information, \"\n",
    "            \"produce a concise final answer.\"\n",
    "        )\n",
    "    )\n",
    "    messages = [system] + state[\"messages\"]\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": [ai_message],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce437e",
   "metadata": {},
   "source": [
    "## 6. Define Tool Node\n",
    "\n",
    "Node name: `tool_node`\n",
    "\n",
    "Responsibilities:\n",
    "- Read the last `AIMessage`\n",
    "- For each `tool_call` requested by the LLM:\n",
    "  - Run the corresponding Python tool\n",
    "  - Append a `ToolMessage` with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"Executes any tool calls requested by the LLM.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    if not getattr(last_msg, \"tool_calls\", None):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    tool_messages: List[ToolMessage] = []\n",
    "\n",
    "    for tool_call in last_msg.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool = tools_by_name[tool_name]\n",
    "        args = tool_call[\"args\"]\n",
    "\n",
    "        observation = tool.invoke(args)\n",
    "\n",
    "        if isinstance(observation, BaseModel):\n",
    "            content = observation.model_dump_json(indent=2)\n",
    "        else:\n",
    "            content = str(observation)\n",
    "\n",
    "        tool_messages.append(\n",
    "            ToolMessage(\n",
    "                content=content,\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": tool_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea347c4",
   "metadata": {},
   "source": [
    "## 7. Define Conditional Edge: Continue or End\n",
    "\n",
    "Logic:\n",
    "- If last `AIMessage` has `tool_calls` â†’ go to `tool_node`\n",
    "- Else â†’ `END` (final answer is ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062678c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Route based on whether the last AIMessage has tool calls.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"tool_node\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3a285",
   "metadata": {},
   "source": [
    "## 8. Build & Compile the LangGraph Workflow\n",
    "\n",
    "Graph:\n",
    "- `START â†’ llm_node â†’ (conditional: tool_node or END)`\n",
    "- `tool_node â†’ llm_node` (loop until no further tool calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"llm_node\", llm_node)\n",
    "builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"llm_node\")\n",
    "builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END],\n",
    ")\n",
    "builder.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "communication_agent = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1mwaq5q7h",
   "metadata": {},
   "source": [
    "## 8.1 Visualize the Graph\n",
    "\n",
    "Display the compiled LangGraph workflow as a diagram to understand the agent's flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aawgfws45w",
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import Image, display, SVG\nimport io\n\nprint(\"ðŸŽ¨ Generating LangGraph Visualization...\\n\")\n\n# Method 1: Try PNG rendering (best quality)\ntry:\n    graph_data = communication_agent.get_graph().draw_mermaid_png()\n    display(Image(graph_data))\n    print(\"âœ… Displayed as PNG image\")\nexcept Exception as e:\n    print(f\"âš ï¸  PNG rendering failed: {e}\\n\")\n    \n    # Method 2: Try ASCII art visualization (always works!)\n    try:\n        from langgraph.graph import Graph\n        \n        print(\"ðŸ“Š LangGraph Workflow Structure:\")\n        print(\"=\" * 70)\n        \n        # Display the graph structure as ASCII\n        graph = communication_agent.get_graph()\n        \n        # Get nodes and edges info\n        print(\"\\nðŸ”µ NODES:\")\n        print(\"  â€¢ START (entry point)\")\n        print(\"  â€¢ llm_node (LLM decision maker)\")\n        print(\"  â€¢ tool_node (execute tools)\")\n        print(\"  â€¢ END (exit point)\")\n        \n        print(\"\\nðŸ”— EDGES:\")\n        print(\"  START â†’ llm_node\")\n        print(\"  llm_node â†’ tool_node (if tools needed)\")\n        print(\"  llm_node â†’ END (if complete)\")\n        print(\"  tool_node â†’ llm_node (loop back)\")\n        \n        print(\"\\n\" + \"=\" * 70)\n        \n        # Try to display mermaid code for manual visualization\n        print(\"\\nðŸ“‹ Mermaid Diagram Code (copy to https://mermaid.live):\")\n        print(\"-\" * 70)\n        mermaid_code = graph.draw_mermaid()\n        print(mermaid_code)\n        print(\"-\" * 70)\n        print(\"\\nðŸ’¡ Tip: Copy the code above and paste at https://mermaid.live to see the visual diagram\")\n        \n    except Exception as e2:\n        print(f\"âŒ Visualization failed: {e2}\")\n        print(\"\\nðŸ“ Graph exists but cannot be visualized in this environment\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Graph compilation successful! The agent is ready to use.\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "id": "p8akl86fj",
   "source": "# Alternative: Create a visual diagram using matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n\ndef draw_langgraph_diagram():\n    \"\"\"Draw the LangGraph workflow as a visual diagram\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.axis('off')\n    \n    # Define node positions\n    nodes = {\n        'START': (5, 9),\n        'llm_node': (5, 6.5),\n        'tool_node': (2, 4),\n        'END': (8, 4)\n    }\n    \n    # Define node colors\n    colors = {\n        'START': '#90EE90',\n        'llm_node': '#87CEEB',\n        'tool_node': '#FFB6C1',\n        'END': '#FFA07A'\n    }\n    \n    # Draw nodes\n    for node_name, (x, y) in nodes.items():\n        if node_name in ['START', 'END']:\n            circle = plt.Circle((x, y), 0.4, color=colors[node_name], ec='black', linewidth=2, zorder=3)\n            ax.add_patch(circle)\n            ax.text(x, y, node_name, ha='center', va='center', fontsize=10, fontweight='bold', zorder=4)\n        else:\n            box = FancyBboxPatch((x-0.6, y-0.35), 1.2, 0.7, \n                                boxstyle=\"round,pad=0.1\", \n                                facecolor=colors[node_name],\n                                edgecolor='black', linewidth=2, zorder=3)\n            ax.add_patch(box)\n            ax.text(x, y, node_name, ha='center', va='center', fontsize=10, fontweight='bold', zorder=4)\n    \n    # Draw edges with arrows\n    # START -> llm_node\n    arrow1 = FancyArrowPatch(\n        (5, 8.6), (5, 6.85),\n        arrowstyle='->', mutation_scale=30, linewidth=2,\n        color='black', zorder=2\n    )\n    ax.add_patch(arrow1)\n    \n    # llm_node -> tool_node (conditional)\n    arrow2 = FancyArrowPatch(\n        (4.4, 6.15), (2.6, 4.35),\n        arrowstyle='->', mutation_scale=30, linewidth=2,\n        color='blue', linestyle='--', zorder=2\n    )\n    ax.add_patch(arrow2)\n    ax.text(3, 5.5, 'if tools\\nneeded', ha='center', va='center', \n            fontsize=8, color='blue', style='italic',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # llm_node -> END (conditional)\n    arrow3 = FancyArrowPatch(\n        (5.6, 6.15), (7.4, 4.35),\n        arrowstyle='->', mutation_scale=30, linewidth=2,\n        color='green', linestyle='--', zorder=2\n    )\n    ax.add_patch(arrow3)\n    ax.text(7, 5.5, 'if complete', ha='center', va='center',\n            fontsize=8, color='green', style='italic',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # tool_node -> llm_node (loop back)\n    arrow4 = FancyArrowPatch(\n        (2, 4.35), (4.4, 6.15),\n        arrowstyle='->', mutation_scale=30, linewidth=2,\n        color='purple', connectionstyle=\"arc3,rad=0.3\", zorder=2\n    )\n    ax.add_patch(arrow4)\n    ax.text(2.5, 5.8, 'return\\nresults', ha='center', va='center',\n            fontsize=8, color='purple', style='italic',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # Add title and legend\n    ax.set_title('Communication & Automation Agent - LangGraph Workflow', \n                fontsize=14, fontweight='bold', pad=20)\n    \n    # Add description boxes\n    descriptions = [\n        \"1. User message enters at START\",\n        \"2. LLM analyzes and decides action\",\n        \"3. Tools execute if needed (classify/reply/extract)\",\n        \"4. Results loop back to LLM\",\n        \"5. Final answer sent to END\"\n    ]\n    \n    desc_text = \"\\n\".join(descriptions)\n    ax.text(5, 1.5, desc_text, ha='center', va='center',\n            fontsize=9, bbox=dict(boxstyle='round', facecolor='lightyellow', \n                                 edgecolor='gray', linewidth=1.5, alpha=0.9))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nâœ… Visual diagram displayed above!\")\n    print(\"ðŸ“Š This diagram shows the complete workflow of the Communication Agent\")\n\n# Display the diagram\ntry:\n    draw_langgraph_diagram()\nexcept Exception as e:\n    print(f\"Could not create matplotlib diagram: {e}\")\n    print(\"The agent graph is still functional - visualization is optional.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "16e4ea88",
   "metadata": {},
   "source": [
    "## 9. Test the Communication & Automation Agent\n",
    "\n",
    "We'll send a realistic user message:\n",
    "- Contains complaint\n",
    "- Has urgency\n",
    "- Implies tasks\n",
    "\n",
    "Expectation:\n",
    "- LLM calls `classify_message_tool`\n",
    "- LLM may call `extract_tasks_tool`\n",
    "- LLM calls `generate_reply_tool`\n",
    "- Final AI response summarizes result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = \"\"\"\n",
    "Hi team,\n",
    "\n",
    "I am really unhappy with the delay in resolving my ticket.\n",
    "It's been 3 days since I reported the login issue and I still can't access my account.\n",
    "I need this fixed by tomorrow, otherwise I will escalate this to your management.\n",
    "\n",
    "Regards,\n",
    "Customer\n",
    "\"\"\"\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=test_message)],\n",
    "    \"llm_calls\": 0,\n",
    "}\n",
    "\n",
    "result_state = communication_agent.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f69167",
   "metadata": {},
   "source": [
    "## 10. Inspect the Conversation Trace\n",
    "\n",
    "This will print:\n",
    "- `HumanMessage`\n",
    "- `AIMessage`(s) (including tool call requests)\n",
    "- `ToolMessage`(s) with JSON tool outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e864af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in result_state[\"messages\"]:\n",
    "    if isinstance(m, HumanMessage):\n",
    "        print(\"ðŸ§‘ HUMAN:\\n\", m.content, \"\\n\")\n",
    "    elif isinstance(m, AIMessage):\n",
    "        print(\"ðŸ¤– AI:\\n\", m.content or \"[Tool call message]\", \"\\n\")\n",
    "        if getattr(m, \"tool_calls\", None):\n",
    "            print(\"  â†³ Tool calls:\", m.tool_calls, \"\\n\")\n",
    "    elif isinstance(m, ToolMessage):\n",
    "        print(\"ðŸ› ï¸ TOOL RESULT:\\n\", m.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64900876",
   "metadata": {},
   "source": [
    "## 11. High-Level Helper for Integration\n",
    "\n",
    "You can use this helper in a FastAPI route:\n",
    "\n",
    "**POST** `/communication-agent`\n",
    "```json\n",
    "{ \"message\": \"...\" }\n",
    "```\n",
    "\n",
    "This helper will:\n",
    "- Run the LangGraph agent\n",
    "- Return the final reply + raw history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f15eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_communication_agent(message: str) -> Dict[str, Any]:\n",
    "    \"\"\"High-level helper to run the Communication & Automation Agent.\n",
    "    Returns:\n",
    "      - final_reply: last AI content message\n",
    "      - history: full message list (for debug/audit)\n",
    "      - llm_calls: number of LLM invocations\n",
    "    \"\"\"\n",
    "    state = communication_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=message)],\n",
    "            \"llm_calls\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    final_reply = None\n",
    "    for m in reversed(state[\"messages\"]):\n",
    "        if isinstance(m, AIMessage) and m.content:\n",
    "            final_reply = m.content\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"final_reply\": final_reply,\n",
    "        \"history\": state[\"messages\"],\n",
    "        \"llm_calls\": state[\"llm_calls\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "if __name__ == \"__main__\":\n",
    "    demo_out = run_communication_agent(\n",
    "        \"Hey, my invoice amount is wrong, please correct it before tomorrow.\"\n",
    "    )\n",
    "    print(\"=== FINAL REPLY ===\")\n",
    "    print(demo_out[\"final_reply\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}